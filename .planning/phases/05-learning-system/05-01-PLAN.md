---
phase: 05-learning-system
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - Fusion-360-MCP-Server/feedback_learning/__init__.py
  - Fusion-360-MCP-Server/feedback_learning/feedback_store.py
  - Fusion-360-MCP-Server/feedback_learning/recency_weighting.py
  - Fusion-360-MCP-Server/feedback_learning/confidence_adjuster.py
  - Fusion-360-MCP-Server/feedback_learning/context_matcher.py
autonomous: true

must_haves:
  truths:
    - "Feedback events can be stored in SQLite with full context snapshots"
    - "Matching feedback can be queried by operation_type + material + geometry_type"
    - "Recent feedback is weighted more heavily than old feedback via exponential decay"
    - "Confidence scores are adjusted only after 3+ samples exist for a context"
    - "Explicit feedback (good/bad) counts 2x weight compared to implicit accept/reject"
    - "Confidence never drops below 0.20 floor"
  artifacts:
    - path: "Fusion-360-MCP-Server/feedback_learning/__init__.py"
      provides: "Module exports for feedback_learning package"
      contains: "record_feedback"
    - path: "Fusion-360-MCP-Server/feedback_learning/feedback_store.py"
      provides: "SQLite storage and retrieval for feedback events"
      contains: "cam_feedback_history"
    - path: "Fusion-360-MCP-Server/feedback_learning/recency_weighting.py"
      provides: "Exponential decay time weighting"
      contains: "calculate_recency_weight"
    - path: "Fusion-360-MCP-Server/feedback_learning/confidence_adjuster.py"
      provides: "Acceptance rate confidence calculation"
      contains: "adjust_confidence_from_feedback"
    - path: "Fusion-360-MCP-Server/feedback_learning/context_matcher.py"
      provides: "Field-based feedback querying"
      contains: "get_matching_feedback"
  key_links:
    - from: "Fusion-360-MCP-Server/feedback_learning/confidence_adjuster.py"
      to: "Fusion-360-MCP-Server/feedback_learning/recency_weighting.py"
      via: "import calculate_recency_weight for weighted acceptance rate"
      pattern: "from .recency_weighting import"
    - from: "Fusion-360-MCP-Server/feedback_learning/confidence_adjuster.py"
      to: "Fusion-360-MCP-Server/feedback_learning/context_matcher.py"
      via: "uses get_matching_feedback to retrieve history before adjusting"
      pattern: "get_matching_feedback"
    - from: "Fusion-360-MCP-Server/feedback_learning/feedback_store.py"
      to: "SQLite via MCP bridge"
      via: "mcp_call_func('sqlite', ...) with tool_unlock_token"
      pattern: "tool_unlock_token.*29e63eb5"
---

<objective>
Build the feedback learning module foundation with SQLite storage, exponential decay recency weighting, field-based context matching, and confidence adjustment logic.

Purpose: This is the core data and computation layer that the MCP handlers (Plan 02) will call. It follows the same pattern as Phase 3 preference_store.py and Phase 4 strategy_preferences.py but adds temporal weighting and acceptance rate calculations.

Output: `feedback_learning/` module with four files: feedback_store.py, recency_weighting.py, confidence_adjuster.py, context_matcher.py, plus __init__.py exports.
</objective>

<execution_context>
@C:\Users\cdeit\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\cdeit\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md

Key reference files for patterns:
@Fusion-360-MCP-Server/stock_suggestions/preference_store.py
@Fusion-360-MCP-Server/toolpath_strategy/strategy_preferences.py
@Fusion-360-MCP-Server/stock_suggestions/__init__.py
@Fusion-360-MCP-Server/toolpath_strategy/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create feedback_store.py with SQLite schema, record_feedback, and feedback statistics</name>
  <files>
    Fusion-360-MCP-Server/feedback_learning/feedback_store.py
  </files>
  <action>
Create `Fusion-360-MCP-Server/feedback_learning/feedback_store.py` following the exact pattern from `stock_suggestions/preference_store.py` and `toolpath_strategy/strategy_preferences.py`.

**SQLite schema** - `cam_feedback_history` table:
```sql
CREATE TABLE IF NOT EXISTS cam_feedback_history (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    operation_type TEXT NOT NULL,
    material TEXT NOT NULL,
    geometry_type TEXT NOT NULL,
    context_snapshot TEXT NOT NULL,
    suggestion_payload TEXT NOT NULL,
    user_choice TEXT,
    feedback_type TEXT NOT NULL,
    feedback_note TEXT,
    confidence_before REAL,
    created_at TIMESTAMP DEFAULT (datetime('now'))
);
```
- operation_type: 'stock_setup', 'toolpath_strategy', 'tool_selection'
- material: Normalized lowercase (e.g., 'aluminum')
- geometry_type: 'pocket-heavy', 'hole-heavy', 'mixed', 'simple'
- context_snapshot: JSON TEXT — full context (bounding_box, features, part size)
- suggestion_payload: JSON TEXT — what was suggested (full response)
- user_choice: JSON TEXT or NULL (null = accepted suggestion as-is)
- feedback_type: 'implicit_accept', 'implicit_reject', 'explicit_good', 'explicit_bad'
- feedback_note: Optional text explaining override
- confidence_before: Confidence score of original suggestion

Also create indexes in separate CREATE INDEX IF NOT EXISTS statements (SQLite does not support inline INDEX in CREATE TABLE):
- `idx_feedback_material_geometry` on (material, geometry_type)
- `idx_feedback_operation_type` on (operation_type)
- `idx_feedback_created_at` on (created_at DESC)

**Functions to implement:**

1. `initialize_feedback_schema(mcp_call_func) -> bool`
   - Creates table + indexes. Safe to call multiple times.
   - Use `SQLITE_TOOL_UNLOCK_TOKEN = "29e63eb5"` constant.
   - Follow exact pattern from `initialize_schema()` in preference_store.py.

2. `record_feedback(operation_type, material, geometry_type, context, suggestion, user_choice, feedback_type, note, mcp_call_func) -> bool`
   - Normalize material and geometry_type to lowercase
   - Serialize context and suggestion dicts to JSON with `json.dumps(obj, sort_keys=True)`
   - user_choice: serialize to JSON if not None, else store NULL
   - Extract confidence_before from `suggestion.get("confidence_score")`
   - Write immediately to SQLite (no batching, per CONTEXT.md decision)
   - Return True on success, False on error
   - Wrap in try/except, never raise

3. `get_feedback_statistics(operation_type=None, mcp_call_func) -> Dict`
   - Overall stats: total_count, accept_count, acceptance_rate
   - Per-material breakdown: material, count, acceptance_pct
   - Per-geometry_type breakdown: geometry_type, count, acceptance_pct
   - Per-operation_type breakdown: operation_type, count, acceptance_pct
   - Optional filter by operation_type
   - Use parameterized queries (not string interpolation) for any filters
   - Return dict with "overall", "by_material", "by_geometry_type", "by_operation_type" keys

4. `export_feedback_history(format, operation_type=None, mcp_call_func) -> str`
   - format: 'csv' or 'json'
   - Query all rows (optionally filtered by operation_type)
   - For CSV: use csv.DictWriter with StringIO
   - For JSON: use json.dumps with indent=2
   - Return formatted string

5. `clear_feedback_history(operation_type=None, mcp_call_func) -> Dict`
   - Per-category reset capability (CONTEXT.md decision)
   - If operation_type provided, delete only matching rows
   - If None, delete all rows
   - Return {"deleted_count": N, "operation_type": type_or_"all"}

**Module docstring** should match the style of preference_store.py — explain what the module does, reference CONTEXT.md decisions, and list the functions.
  </action>
  <verify>
Read the file and confirm:
1. Schema has all columns including created_at with datetime('now')
2. Indexes are separate CREATE INDEX statements
3. record_feedback serializes JSON with sort_keys=True
4. get_feedback_statistics returns dict with four breakdown keys
5. export_feedback_history supports both csv and json
6. clear_feedback_history supports per-category reset
7. All functions accept mcp_call_func parameter and use SQLITE_TOOL_UNLOCK_TOKEN
8. All SQLite operations use parameterized queries (? placeholders)
  </verify>
  <done>
feedback_store.py exists with initialize_feedback_schema, record_feedback, get_feedback_statistics, export_feedback_history, and clear_feedback_history functions. All follow the MCP SQLite bridge pattern from preference_store.py.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create recency_weighting.py, confidence_adjuster.py, context_matcher.py, and __init__.py</name>
  <files>
    Fusion-360-MCP-Server/feedback_learning/recency_weighting.py
    Fusion-360-MCP-Server/feedback_learning/confidence_adjuster.py
    Fusion-360-MCP-Server/feedback_learning/context_matcher.py
    Fusion-360-MCP-Server/feedback_learning/__init__.py
  </files>
  <action>
**File 1: `recency_weighting.py`**

Pure Python module (no Fusion API dependency, no MCP dependency). Uses only math and datetime from stdlib.

1. `calculate_recency_weight(feedback_timestamp, halflife_days=30.0) -> float`
   - Input: ISO timestamp string from SQLite (e.g., "2026-02-05 14:30:00")
   - Formula: W = e^(-lambda * t) where lambda = ln(2) / halflife_days, t = age in days
   - Use `datetime.fromisoformat()` for parsing (handle 'Z' suffix by replacing with '+00:00')
   - Use `datetime.now(timezone.utc)` for current time (UTC-aware per RESEARCH.md pitfall #6)
   - Clamp result to [0.0, 1.0]
   - Return float

2. `get_weighted_acceptance_rate(feedback_history, halflife_days=30.0) -> Tuple[float, int]`
   - Input: List of feedback event dicts (must have 'created_at' and 'feedback_type' keys)
   - For each event:
     - Calculate recency weight via calculate_recency_weight
     - If feedback_type starts with 'explicit_', multiply weight by 2.0 (CONTEXT.md: explicit 2x)
     - Add weight to weighted_total
     - If feedback_type in ('implicit_accept', 'explicit_good'), add weight to weighted_accepts
   - Return (acceptance_rate, sample_count)
   - If weighted_total < 0.01, return (0.5, 0) as neutral default

**File 2: `confidence_adjuster.py`**

Pure Python module. Imports from recency_weighting.

1. Constants:
   - `MIN_SAMPLES = 3` (CONTEXT.md: require 3+ samples before adjusting)
   - `CONFIDENCE_FLOOR = 0.20` (Research open question #3: prevents death spiral)
   - `FULL_TRUST_SAMPLES = 10` (at 10+ samples, fully trust acceptance rate)
   - `TENTATIVE_THRESHOLD = 0.60` (CONTEXT.md: flag as tentative below this)

2. `adjust_confidence_from_feedback(base_confidence, feedback_history, min_samples=MIN_SAMPLES, halflife_days=30.0) -> Tuple[float, str]`
   - If len(feedback_history) < min_samples: return (base_confidence, 'default_rules')
   - Calculate weighted acceptance rate via get_weighted_acceptance_rate
   - Blend: sample_weight = min(1.0, sample_count / FULL_TRUST_SAMPLES)
   - adjusted = base_confidence * (1 - sample_weight) + acceptance_rate * sample_weight
   - Apply CONFIDENCE_FLOOR: adjusted = max(CONFIDENCE_FLOOR, adjusted)
   - Determine source_tag:
     - If adjusted < TENTATIVE_THRESHOLD: 'user_preference_tentative'
     - Else: 'user_preference'
   - Return (round(adjusted, 2), source_tag)

3. `should_notify_learning(feedback_history) -> bool`
   - Returns True if len(feedback_history) == MIN_SAMPLES (just crossed threshold)
   - This enables first-time learning notification per CONTEXT.md

**File 3: `context_matcher.py`**

Requires MCP bridge for SQLite queries. Follows preference_store.py pattern.

1. `get_matching_feedback(operation_type, material, geometry_type, limit=50, mcp_call_func) -> List[Dict]`
   - Normalize material and geometry_type to lowercase
   - Query cam_feedback_history WHERE:
     - operation_type = ?
     - material LIKE ? (using %material_key% for partial material family matching)
     - geometry_type = ?
   - ORDER BY created_at DESC, LIMIT ?
   - Parse JSON fields back to dicts (context_snapshot, suggestion_payload, user_choice)
   - Return list of feedback event dicts
   - On error, return empty list (never raise)
   - Use SQLITE_TOOL_UNLOCK_TOKEN = "29e63eb5"

2. `get_conflicting_choices(feedback_history) -> List[Dict]`
   - Pure Python function, no MCP needed
   - Analyze feedback_history for conflicting patterns
   - Group by user_choice (serialized as JSON for comparison)
   - If multiple distinct choices exist, return them as alternatives
   - For each alternative: include choice, count, most_recent_date, weighted_score
   - Return empty list if no conflicts
   - Per CONTEXT.md: "Show conflicting choices as multiple options"

**File 4: `__init__.py`**

Follow the exact pattern from `toolpath_strategy/__init__.py` and `stock_suggestions/__init__.py`.

Module docstring explaining the feedback learning system.

Imports:
```python
from .feedback_store import (
    initialize_feedback_schema,
    record_feedback,
    get_feedback_statistics,
    export_feedback_history,
    clear_feedback_history,
    FEEDBACK_HISTORY_SCHEMA
)
from .recency_weighting import (
    calculate_recency_weight,
    get_weighted_acceptance_rate
)
from .confidence_adjuster import (
    adjust_confidence_from_feedback,
    should_notify_learning,
    MIN_SAMPLES,
    CONFIDENCE_FLOOR,
    TENTATIVE_THRESHOLD
)
from .context_matcher import (
    get_matching_feedback,
    get_conflicting_choices
)

__all__ = [
    # Feedback storage
    "initialize_feedback_schema",
    "record_feedback",
    "get_feedback_statistics",
    "export_feedback_history",
    "clear_feedback_history",
    "FEEDBACK_HISTORY_SCHEMA",
    # Recency weighting
    "calculate_recency_weight",
    "get_weighted_acceptance_rate",
    # Confidence adjustment
    "adjust_confidence_from_feedback",
    "should_notify_learning",
    "MIN_SAMPLES",
    "CONFIDENCE_FLOOR",
    "TENTATIVE_THRESHOLD",
    # Context matching
    "get_matching_feedback",
    "get_conflicting_choices",
]
```

IMPORTANT: All four files must use relative imports (from .module_name import ...) since this is a package loaded as part of the Fusion 360 add-in (lesson from Phase 3).
  </action>
  <verify>
Read all four files and confirm:
1. recency_weighting.py: calculate_recency_weight uses e^(-lambda*t), get_weighted_acceptance_rate handles 2x explicit weight
2. confidence_adjuster.py: MIN_SAMPLES=3, CONFIDENCE_FLOOR=0.20, TENTATIVE_THRESHOLD=0.60 constants defined; adjust_confidence_from_feedback returns tuple; should_notify_learning checks == MIN_SAMPLES
3. context_matcher.py: get_matching_feedback uses LIKE for material family matching; get_conflicting_choices groups by user_choice
4. __init__.py: All exports listed in __all__, all imports use relative syntax (from .module)
5. No Fusion API imports in any file except context_matcher.py which only uses MCP bridge
6. recency_weighting.py and confidence_adjuster.py are pure Python with only stdlib imports
  </verify>
  <done>
feedback_learning/ package exists with four modules. recency_weighting.py and confidence_adjuster.py are pure Python. context_matcher.py uses MCP bridge. __init__.py exports all public APIs. All use relative imports.
  </done>
</task>

</tasks>

<verification>
1. All five files exist in Fusion-360-MCP-Server/feedback_learning/
2. __init__.py successfully exports all public APIs
3. feedback_store.py schema matches RESEARCH.md pattern with all columns
4. recency_weighting.py has no external dependencies (math + datetime only)
5. confidence_adjuster.py imports from recency_weighting (cross-module link)
6. context_matcher.py follows preference_store.py query pattern
7. All MCP SQLite calls use tool_unlock_token "29e63eb5"
8. All functions handle errors gracefully (try/except, return defaults)
</verification>

<success_criteria>
- feedback_learning/ module exists with 5 files
- SQLite schema creates cam_feedback_history table with all required columns
- Exponential decay formula W = e^(-lambda*t) correctly implemented
- Confidence adjustment requires 3+ samples and has 0.20 floor
- Explicit feedback gets 2x weight in acceptance rate calculation
- Field-based matching uses LIKE for material family matching
- All code follows existing codebase patterns (relative imports, error handling, docstrings)
</success_criteria>

<output>
After completion, create `.planning/phases/05-learning-system/05-01-SUMMARY.md`
</output>
